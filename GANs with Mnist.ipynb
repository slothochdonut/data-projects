{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Dense, \n",
    "                                    BatchNormalization,\n",
    "                                    LeakyReLU,\n",
    "                                    Reshape,\n",
    "                                    Conv2DTranspose,\n",
    "                                    Conv2D,\n",
    "                                    Dropout,\n",
    "                                    Flatten)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# underscore to omit the label arrays\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5)/127.5 #normalize the images to [-1,1]\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "#Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    model.add(Reshape((7,7,256)))\n",
    "    assert model.output_shape == (None, 7,7,256) # Note: None is the batch size\n",
    "    \n",
    "    model.add(Conv2DTranspose(128,(5,5), strides=(1,1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7,7,128) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    model.add(Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14,14,64) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    model.add(Conv2DTranspose(1,(5,5),strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28,28,1)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12544)             1254400   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12544)            50176     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 7, 7, 128)        819200    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 64)       204800    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        1600      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea48b18710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a random noise and generate a sample(untrained)\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "#Visualize the generated image\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (5,5), strides = (2,2), padding='same', input_shape=[28,28,1]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, (5,5), strides = (2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 6273      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method returns a helper function to compute cross rentropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(real_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "num_examples_to_generate = 16\n",
    "noise_dim = 100\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function annotation causes the function \n",
    "# to be \"compiled\" as part of the training\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "  \n",
    "    # 1 - Create a random noise to feed it into the model\n",
    "    # for the image generation\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    # 2 - Generate images and calculate loss values\n",
    "    # GradientTape method records operations for automatic differentiation.\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # 3 - Calculate gradients using loss values and model variables\n",
    "    # \"gradient\" method computes the gradient using \n",
    "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
    "    \n",
    "    # It accepts a target (e.g., gen_loss) variable and \n",
    "    # a source variable (e.g.,generator.trainable_variables)\n",
    "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
    "    # source --> a list or nested structure of Tensors or Variables.\n",
    "    # target will be differentiated against elements in sources.\n",
    "\n",
    "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
    "    # (or IndexedSlices, or None), one for each element in sources. \n",
    "    # Returned structure is the same as the structure of sources.\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
    "                                               generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
    "                                                discriminator.trainable_variables)\n",
    "    \n",
    "    # 4 - Process  Gradients and Run the Optimizer\n",
    "    # \"apply_gradients\" method processes aggregated gradients. \n",
    "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
    "    \"\"\"\n",
    "    Example use of apply_gradients:\n",
    "    grads = tape.gradient(loss, vars)\n",
    "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
    "    # Processing aggregated gradients.\n",
    "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
    "    \"\"\"\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  # 1 - Generate images\n",
    "  predictions = model(test_input, training=False)\n",
    "  # 2 - Plot the generated images\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "  # 3 - Save the generated images\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display # A command shell for interactive computing in Python.\n",
    "\n",
    "def train(dataset, epochs):\n",
    "  # A. For each epoch, do the following:\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    # 1 - For each batch of the epoch, \n",
    "    for image_batch in dataset:\n",
    "      # 1.a - run the custom \"train_step\" function\n",
    "      # we just declared above\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # 2 - Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # 3 - Save the model every 5 epochs as \n",
    "    # a checkpoint, which we will use later\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    # 4 - Print out the completed epoch no. and the time spent\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # B. Generate a final image after the training is completed\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_at_epoch_0060.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7793c09281f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdisplay_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-7793c09281f5>\u001b[0m in \u001b[0;36mdisplay_image\u001b[1;34m(epoch_no)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Display a single image using the epoch number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_at_epoch_0060.png'"
     ]
    }
   ],
   "source": [
    "# PIL is a library which may open different image file formats\n",
    "import PIL \n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # The glob module is used for Unix style pathname pattern expansion.\n",
    "import imageio # The library that provides an easy interface to read and write a wide range of image data\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  # image = imageio.imread(filename)\n",
    "  # writer.append_data(image)\n",
    "  \n",
    "display.Image(open('dcgan.gif','rb').read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
